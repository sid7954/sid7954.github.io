---
layout: about
title:
permalink: 
invisible: true
seo:
  title: Siddhant Garg
---

<a href="mailto:sgarg33@wisc.edu">Email</a> / 
<a href="https://twitter.com/sid7954">Twitter</a> / <a href="https://scholar.google.com/citations?user=V02t618AAAAJ&hl=en&oi=ao">Google Scholar</a> / <a href="https://www.semanticscholar.org/author/Siddhant-Garg/2295877">Semantic Scholar</a> <br />

I am an <a href="https://www.amazon.science/">Applied Scientist II</a> at Amazon working in the Alexa AI Search team on Question Answering systems. My current research interests are towards improving answer ranking and answer generation models for Open Domain Question Answering. Before this, I graduated from the <a href="https://www.wisc.edu/">University of Wisconsin-Madison</a> with a Masters in <a href="https://www.cs.wisc.edu/">Computer Science</a> in May 2020. I completed my <a href="https://minds.wisconsin.edu/bitstream/handle/1793/80196/TR1862%20Siddhant%20Garg.pdf?sequence=1&isAllowed=y">Masters Thesis</a> under the guidance of <a href="http://pages.cs.wisc.edu/~yliang/">Prof. Yingyu Liang</a> on representation learning paradigms and their application to text classification. Prior to this, I completed my Bachelors in <a href="https://www.cse.iitb.ac.in/">Computer Science and Engineering</a> from <a href="https://www.iitb.ac.in/">IIT Bombay</a> in August 2018 where I was advised by <a href="https://www.cse.iitb.ac.in/~sunita/">Prof. Sunita Sarawagi</a>.

### News and Updates
<div style="height:400px;overflow:auto;">
<table>
<col width="100px">
<col width="650px">

<tr><td><b>Oct 2022:</b></td><td> Our <a href="http://arxiv.org/abs/2210.12865">paper</a> on knowledge transfer from answer ranking to answer generation models got accepted at EMNLP 2022</td></tr>
<tr><td><b>Oct 2022:</b></td><td> Our <a href="https://arxiv.org/abs/2205.10455">paper</a> on pre-training transformers by exploiting document structure for answer selection got accepted at EMNLP 2022</td></tr>
<tr><td><b>Jul 2022:</b></td><td> Our <a href="https://arxiv.org/abs/2110.02667">paper</a> on analysis of attentive walk-aggregating GNNs got accepted to Transactions on Machine Learning Research (TMLR) journal</td></tr>
<tr><td><b>Jul 2022:</b></td><td> I attended NAACL 2022 in Seattle, WA to present our paper on multi-sentence inference</td></tr>
<tr><td><b>May 2022:</b></td><td> I attended ACL 2022 in Dublin, Ireland</td></tr>
<tr><td><b>May 2022:</b></td><td> <a href="https://arxiv.org/abs/2205.10455">Pre-print</a> on pre-training transformers by exploiting paragragph and document structure for answer sentence selection released</td></tr>
<tr><td><b>Apr  2022:</b></td><td> Our paper <a href="https://arxiv.org/abs/2205.01228">"Paragraph-based Transformer Pretraining for Multi-Sentence Inference"</a> got accepted at NAACL 2022</td></tr>
<tr><td><b>Nov 2021:</b></td><td> <a href="https://arxiv.org/abs/2110.02667">Pre-print</a> on analysis of attentive walk-aggregating GNNs released</td></tr>  
<tr><td><b>Aug  2021:</b></td><td> Our paper <a href="https://arxiv.org/abs/2109.07009">"Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering"</a> got accepted at EMNLP 2021 as an oral presentation</td></tr>
<tr><td><b>Aug  2021:</b></td><td> Our paper <a href="https://arxiv.org/abs/2101.11214">"Towards Robustness to Label Noise in Text Classification via Noise Modeling"</a> got accepted at CIKM 2021</td></tr>
<tr><td><b>Jul  2021:</b></td><td> I was recognised for being in the Top 10% of the reviewers for ICML 2021</td></tr>
<tr><td><b>Jul  2021:</b></td><td> I was recognised for being an Outstanding Reviewer for ACL 2021</td></tr>
<tr><td><b>Apr 2021:</b></td><td> <a href="https://arxiv.org/abs/2101.11214">Paper</a> on text classification with noisy labels accepted at two ICLR 2021 Workshops: RobustML and S2D-OLAD</td></tr>
<tr><td><b>Jan 2021:</b></td><td> <a href="https://arxiv.org/abs/2101.11214">Pre-print</a> on text classification with noisy labels released</td></tr>
<tr><td><b>Dec 2020:</b></td><td> I was recognised for being an Outstanding Reviewer for EMNLP 2020</td></tr>
<tr><td><b>Oct 2020:</b></td><td> I was recognised for being in the Top 10% of the reviewers for NeurIPS 2020</td></tr>
<tr><td><b>Sep 2020:</b></td><td> Our paper <a href="https://arxiv.org/abs/2008.02447">"Functional Regularization for Representation Learning: A Unified Theoretical Perspective"</a> got accepted to NeurIPS 2020</td></tr>
<tr><td><b>Sep 2020:</b></td><td> Our paper <a href="https://arxiv.org/abs/2004.05119">"BAE: BERT-based Adversarial Examples for Text Classification"</a> got accepted at EMNLP 2020 in the main conference</td></tr>
<tr><td><b>Sep 2020:</b></td><td> Our paper <a href="https://arxiv.org/abs/2004.05119">"Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer"</a> got accepted at AACL 2020</td></tr>
<tr><td><b>Jul  2020:</b></td><td> Our paper <a href="https://arxiv.org/abs/2008.01761">"Can Adversarial Weight Perturbations Inject Neural Backdoors?"</a> got accepted at CIKM 2020</td></tr>
<tr><td><b>Jun 2020:</b></td><td> I joined Amazon Alexa AI Search as an Applied Scientist</td></tr>
<tr><td><b>May 2020:</b></td><td> <a href="https://arxiv.org/abs/2005.04316">Survey</a> on advances in Quantum Deep Learning released</td></tr>
<tr><td><b>May 2020:</b></td><td> Defended my <a href="https://minds.wisconsin.edu/bitstream/handle/1793/80196/TR1862%20Siddhant%20Garg.pdf?sequence=1&isAllowed=y">Master's Thesis</a> on representation learning paradigms and their application to text classification </td></tr>
<tr><td><b>Apr 2020:</b></td><td> <a href="https://arxiv.org/abs/2004.05119">Pre-print</a> on fine-tuning BERT for data scarce text classification released</td></tr>
<tr><td><b>Apr 2020:</b></td><td> <a href="https://arxiv.org/abs/2004.01970">Pre-print</a> on generating adversarial examples for NLP using BERT released</td></tr>
<tr><td><b>Feb 2020:</b></td><td> Presented our <a href="https://arxiv.org/abs/1911.04118">paper</a> on transformers for answer selection at AAAI 2020, New York City </td></tr>
<tr><td><b>Nov 2019:</b></td><td> <a href="https://arxiv.org/abs/1911.04118">Paper</a> on transformers for answer selection accepted for oral presentation at AAAI 2020 </td></tr>
<tr><td><b>Oct 2019:</b></td><td> <a href="https://ieeexplore.ieee.org/abstract/document/8960990">Paper</a> on making inference graphs interpretable for face recognition accepted at IVCNZ 2019, New Zealand</td></tr>
<tr><td><b>Sep 2019:</b></td><td> <a href="https://arxiv.org/abs/1910.01161">Paper</a> on stochastic bandits with delayed composite feedback accepted at NeurIPS 2019 Workshop on ML with Guarantees </td></tr>
<tr><td><b>Jun 2019:</b></td><td> Two posters accepted at the Midwest Machine Learning Symposium(MMLS) 2019 </td></tr>
<tr><td><b>May 2019:</b></td><td> I joined Amazon Alexa Search at Manhattan Beach, CA as an Applied Scientist Intern under <a href="http://disi.unitn.it/~moschitti/">Alessandro Moschitti</a></td></tr>
<tr><td><b>Nov 2018:</b></td><td> My co-author Shiv Shankar presented our paper at EMNLP 2018, Brussels, Belgium</td></tr>
<tr><td><b>Sep 2018:</b></td><td> I joined the Department of Computer Science at the University of Wisconsin-Madison as a Masters student </td></tr>
<tr><td><b>Aug 2018:</b></td><td> Short <a href="https://www.aclweb.org/anthology/D18-1065.pdf">paper</a> on differentiable hard attention for seq2seq learning accepted at EMNLP 2018 </td></tr>
<tr><td><b>May 2018:</b></td><td> Defended my <a href="https://drive.google.com/file/d/1Qj2ymtgOceUwHQKpkU_xkmU5PpZ0SjAf/view?usp=sharing">Bachelor's Thesis</a> on structured attention models for seq2seq learning</td></tr>
</table>
</div>
