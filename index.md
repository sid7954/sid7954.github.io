---
layout: about
title:
permalink: 
invisible: true
seo:
  title: Siddhant Garg
---

<a href="mailto:sgarg33@wisc.edu">Email</a> / <a href="https://scholar.google.com/citations?user=V02t618AAAAJ&hl=en&oi=ao">Google Scholar</a> <br />

I am an Applied Scientist at Amazon working in the Alexa AI Search team on Question Answering systems. I graduated from the University of Wisconsin-Madison with a Masters in Computer Science in May 2020. I completed my Masters Thesis under the guidance of Prof. Yingyu Liang on representation learning paradigms and their application to text classification. Prior to this, I completed my Bachelors in Computer Science and Engineering from IIT Bombay in August 2018 where I was advised by Prof. Sunita Sarawagi.

### News and Updates
<div style="height:400px;overflow:auto;">
<table>
<col width="100px">
<col width="650px">
<tr><td><b>Sep 2020:</b></td><td> Our paper <a href="https://arxiv.org/pdf/2008.02447.pdf">"Functional Regularization for Representation Learning: A Unified Theoretical Perspective"</a> got accepted at NeurIPS 2020 as a Poster</td></tr>
<tr><td><b>Sep 2020:</b></td><td> Our paper <a href="https://arxiv.org/abs/2004.05119">"BAE: BERT-based Adversarial Examples for Text Classification"</a> got accepted at EMNLP 2020 in the main conference</td></tr>
<tr><td><b>Sep 2020:</b></td><td> Our paper "Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer" got accepted at AACL 2020. Link coming soon!</td></tr>
<tr><td><b>Jul 2020:</b></td><td> Our paper "Can Adversarial Weight Perturbations Inject Neural Backdoors?" got accepted at CIKM 2020. Preprint coming soon!</td></tr>
<tr><td><b>Jun 2020:</b></td><td> I joined Amazon Alexa AI Search as an Applied Scientist</td></tr>
<tr><td><b>May 2020:</b></td><td> <a href="https://arxiv.org/abs/2005.04316">Survey</a> on advances in Quantum Deep Learning released</td></tr>
<tr><td><b>May 2020:</b></td><td> Defended my <a href="https://minds.wisconsin.edu/bitstream/handle/1793/80196/TR1862%20Siddhant%20Garg.pdf?sequence=1&isAllowed=y">Master's Thesis</a> on representation learning paradigms and their application to text classification </td></tr>
<tr><td><b>Apr 2020:</b></td><td> <a href="https://arxiv.org/abs/2004.05119">Pre-print</a> on fine-tuning BERT for data scarce text classification released</td></tr>
<tr><td><b>Apr 2020:</b></td><td> <a href="https://arxiv.org/abs/2004.01970">Pre-print</a> on generating adversarial examples for NLP using BERT released</td></tr>
<tr><td><b>Feb 2020:</b></td><td> Presented our <a href="https://arxiv.org/abs/1911.04118">paper</a> on transformers for answer selection at AAAI 2020, New York City </td></tr>
<tr><td><b>Nov 2019:</b></td><td> <a href="https://arxiv.org/abs/1911.04118">Paper</a> on transformers for answer selection accepted for oral presentation at AAAI 2020 </td></tr>
<tr><td><b>Oct 2019:</b></td><td> <a href="https://ieeexplore.ieee.org/abstract/document/8960990">Paper</a> on making inference graphs interpretable for face recognition accepted at IVCNZ 2019, New Zealand</td></tr>
<tr><td><b>Sep 2019:</b></td><td> <a href="https://arxiv.org/abs/1910.01161">Paper</a> on stochastic bandits with delayed composite feedback accepted at NeurIPS 2019 Workshop on ML with Guarantees </td></tr>
<tr><td><b>Jun 2019:</b></td><td> Two posters accepted at the Midwest Machine Learning Symposium(MMLS) 2019 </td></tr>
<tr><td><b>May 2019:</b></td><td> I joined Amazon Alexa Search at Manhattan Beach, CA as an Applied Scientist Intern under <a href="http://disi.unitn.it/~moschitti/">Alessandro Moschitti</a></td></tr>
<tr><td><b>Nov 2018:</b></td><td> My co-author Shiv Shankar presented our paper at EMNLP 2018, Brussels, Belgium</td></tr>
<tr><td><b>Sep 2018:</b></td><td> I joined the Department of Computer Science at the University of Wisconsin-Madison as a Masters student </td></tr>
<tr><td><b>Aug 2018:</b></td><td> Short <a href="https://www.aclweb.org/anthology/D18-1065.pdf">paper</a> on differentiable hard attention for seq2seq learning accepted at EMNLP 2018 </td></tr>
<tr><td><b>May 2018:</b></td><td> Defended my <a href="https://drive.google.com/file/d/1Qj2ymtgOceUwHQKpkU_xkmU5PpZ0SjAf/view?usp=sharing">Bachelor's Thesis</a> on structured attention models for seq2seq learning</td></tr>
</table>
</div>
